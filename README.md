# Mini Local Chatbot (llama.cpp + Gradio)

This project is a lightweight local chatbot built using **llama.cpp** with a **Gradio** web interface.  
It supports multiple open-source language models, allows switching between them instantly, and includes full chat-history management â€” all running **fully offline** on your own machine.

## ğŸš€ Features

### ğŸ§  Multiple Local Models (via llama.cpp)

- **TinyLlama-1.1B-Chat (Q8_0)** â€” ultra-fast and lightweight  
- **Mistral-7B-Instruct (Q2_K)** â€” stronger reasoning with low memory usage  
- **DeepSeek-R1-Qwen3-8B (Q4_K_XL)** â€” deeper answers with slower speed  

### ğŸŒ Gradio Web UI

- Clean, responsive browser interface  
- One-click model switching  
- Smooth message display  

### ğŸ’¾ Chat History Management

- Download current conversation as JSON  
- Upload & load past chat histories  
- All files saved inside the `history/` directory  

### ğŸ”’ 100% Local Execution

- No API calls, no network dependency  
- Ideal for private, offline, or on-device use  
- No API calls, no network dependency
- Ideal for private, offline, or on-device use

## ğŸ¥ Demo GIFs
### ğŸ’¬ Ask the Chatbot
![ask_chatbot](https://github.com/user-attachments/assets/0d1ebf6d-80b8-4359-9efa-7df206973a95)

### ğŸ“¥ Download Chat History
![download_history](https://github.com/user-attachments/assets/d52c3b94-c83f-4050-a15c-bb89a3fa928e)

### ğŸ“¤ Upload Chat History
![upload_history](https://github.com/user-attachments/assets/0d442daf-d37e-4933-99af-9553138bcc0e)

### ğŸ”€ Switch Models
![switch_model](https://github.com/user-attachments/assets/50ce08fa-87bf-4fd4-9742-e49aa9cd5333)

## ğŸ“ Repository Structure
mini_chatbot/
â”œâ”€â”€ app.py                 # Main launcher for the Gradio UI  
â”œâ”€â”€ chatbot.py             # Backend logic + llama.cpp wrapper  
â”œâ”€â”€ download.sh            # Downloads all model files  
â”œâ”€â”€ requirements.txt       # Python dependencies  
â”œâ”€â”€ history/               # Stored chat histories (JSON)  
â””â”€â”€ ui/                    # UI helper components  

## ğŸ›  Installation
1. Clone the repository
`git clone https://github.com/your-username/mini_chatbot.git`
`cd mini_chatbot`

2. (Optional) Create a virtual environment

`python -m venv .venv
source .venv/bin/activate`

3. Install dependencies
`pip install -r requirements.txt`

4. Download models
Use the included script:

`chmod +x download.sh
./download.sh`


This fetches all required GGUF model files into the proper folders.  

## â–¶ï¸ Running the Chatbot
Start the Gradio UI:
`python app.py`

Then open:
`http://127.0.0.1:7860`


You can now:
âœ” Select a model  
âœ” Chat normally  
âœ” Save / load chat history  
âœ” Switch models mid-session  

## ğŸ“ Usage Notes

- TinyLlama is best for speed and quick replies.

- Mistral < Qwen are better for quality and depth, but will take more processing time, especially on CPU.

- History files are standard JSONL format and editable.

- All computation is local â€” suitable for private or offline applications.
